Markdown

# ğŸ’» TP1 : Programmation des RDDs avec Apache Spark

## Table des MatiÃ¨res
* [ğŸš€ Introduction](#-introduction)
* [ğŸ¯ Objectifs du TP](#-objectifs-du-tp)
* [âš™ï¸ Technologies UtilisÃ©es](#-technologies-utilisÃ©es)
* [ğŸ“‚ Structure du Projet](#-structure-du-projet)
* [ğŸ’¡ Exercice 1 : Analyse des Ventes (DonnÃ©es StructurÃ©es)](#-exercice-1--analyse-des-ventes-donnÃ©es-structurÃ©es)
* [ğŸ“Š Exercice 2 : Analyse de Fichiers de Logs (DonnÃ©es Semi-structurÃ©es)](#-exercice-2--analyse-de-fichiers-de-logs-donnÃ©es-semi-structurÃ©es)
* [â–¶ï¸ Comment ExÃ©cuter le Projet](#-comment-exÃ©cuter-le-projet)

---

## ğŸš€ Introduction

Ce projet est le **Travail Pratique nÂ°1** sur la programmation distribuÃ©e avec **Apache Spark**. Il vise Ã  maÃ®triser l'utilisation des **RDD (Resilient Distributed Datasets)** pour le traitement de donnÃ©es massives (**Big Data**).

Nous explorons deux cas d'usage fondamentaux : l'agrÃ©gation de donnÃ©es structurÃ©es de ventes, et le *parsing* et l'analyse de fichiers de logs Apache. L'objectif est de dÃ©montrer l'efficacitÃ© des transformations (`map`, `filter`, `flatMap`) et des actions (`reduceByKey`, `count`) des RDDs en environnement distribuÃ©.

---

## ğŸ¯ Objectifs du TP

* Comprendre et implÃ©menter le concept de **RDD** de Spark.
* Appliquer les transformations `map`, `filter`, `reduceByKey` pour l'agrÃ©gation de donnÃ©es.
* DÃ©velopper des applications Spark en mode **local**.
* Traiter diffÃ©rents formats de donnÃ©es : structurÃ© (ventes) et semi-structurÃ© (logs).
* Calculer des indicateurs clÃ©s (KPIs) : totaux, pourcentages d'erreurs, Top N.

---

## âš™ï¸ Technologies UtilisÃ©es

| Technologie | Version | Description |
| :--- | :--- | :--- |
| **Apache Spark** | [Ã€ complÃ©ter : ex. 3.4.0] | Moteur d'analyse unifiÃ© pour le traitement de donnÃ©es Ã  grande Ã©chelle. |
| **Langage de Dev.** | **Java** | Langage utilisÃ© pour la programmation des applications Spark. |
| **SystÃ¨me de Build** | [Ã€ complÃ©ter : ex. Maven/Gradle] | Pour la gestion des dÃ©pendances et la construction du JAR. |

---

## ğŸ“‚ Structure du Projet
<img width="702" height="685" alt="image" src="https://github.com/user-attachments/assets/d4ee22b7-6372-4e55-9e29-e3d9de307fcb" />


---

## ğŸ’¡ Exercice 1 : Analyse des Ventes (DonnÃ©es StructurÃ©es)

### Fichier d'entrÃ©e
`data/ventes.txt` (Structure : `date ville produit prix`)

### Travaux RÃ©alisÃ©s
1.  **Total des ventes par ville** : Calcul de la somme des prix pour chaque ville unique en utilisant `map` et `reduceByKey`.
2.  **Prix total des ventes par ville et par annÃ©e** : Extension du travail prÃ©cÃ©dent pour inclure l'annÃ©e dans la clÃ© d'agrÃ©gation.

---

## ğŸ“Š Exercice 2 : Analyse de Fichiers de Logs (DonnÃ©es Semi-structurÃ©es)

### Fichier d'entrÃ©e
`data/access.log` (Format Apache combinÃ©)

### Travaux RÃ©alisÃ©s
1.  **Extraction de champs** : ImplÃ©mentation d'une fonction de *parsing* robuste (via Regex) pour isoler les 6 champs demandÃ©s (IP, date/heure, mÃ©thode, ressource, code HTTP, taille de la rÃ©ponse).
2.  **Statistiques de base** : Calcul du nombre total de requÃªtes et du pourcentage de requÃªtes en erreur (codes $\geq 400$).
3.  **Top N** : DÃ©termination des 5 adresses IP les plus actives et des 5 ressources les plus demandÃ©es.
4.  **RÃ©partition par code HTTP** : Comptage du nombre de requÃªtes pour chaque code de statut HTTP.

---

## â–¶ï¸ Comment ExÃ©cuter le Projet

### PrÃ©requis
* Installation de **Java JDK** (*[Ã€ complÃ©ter : ex. 17]*).
* Installation et configuration de **Apache Spark** (*[Ã€ complÃ©ter : assurez-vous que `spark-submit` est disponible]*).
* Outil de build : **Maven** ou **Gradle**.

### Ã‰tapes d'exÃ©cution

1.  **Cloner le dÃ©pÃ´t :**
    ```bash
    git clone [URL_DE_VOTRE_REPO]
    cd [NOM_DU_REPO]
    ```

2.  **Construire le fichier JAR :**
    *(Exemple avec Maven)*
    ```bash
    mvn clean package
    ```

3.  **Lancer l'application :**
    Utilisez `spark-submit` pour exÃ©cuter les classes Java.

    *Exemple pour l'Analyse des Logs :*
    ```bash
    spark-submit \
      --class Ex2_Logs.AnalyseLogs \
      --master local[*] \
      target/[NOM_DU_FICHIER_JAR].jar \
      ./data/access.log
    ```
    *(Ajustez le `--class` et le nom du fichier JAR en fonction de vos conventions d
